# BÃO CÃO CHI TIáº¾T: PhÃ¢n TÃ­ch Quan Äiá»ƒm Sinh ViÃªn UIT
## Vietnamese Student Feedback Sentiment Analysis - Detailed Technical Report

**Sinh viÃªn:** [Há» tÃªn sinh viÃªn]  
**MSSV:** [MÃ£ sá»‘ sinh viÃªn]  
**MÃ´n há»c:** Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP)

---

# PHáº¦N 1: Ká»¸ THUáº¬T Sá»¬ Dá»¤NG

## 1.1. Tá»•ng quan phÆ°Æ¡ng phÃ¡p

Dá»± Ã¡n triá»ƒn khai **9 phÆ°Æ¡ng phÃ¡p** phÃ¢n loáº¡i quan Ä‘iá»ƒm, chia thÃ nh 4 nhÃ³m:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHÆ¯Æ NG PHÃP TIáº¾P Cáº¬N                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Traditional ML â”‚  Deep Learning  â”‚  Hybrid & Ensemble     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ SVM + TF-IDF â”‚  â€¢ PhoBERT-base â”‚  â€¢ Hybrid V1 (Fusion)  â”‚
â”‚  â€¢ Logistic Reg â”‚  â€¢ PhoBERT-largeâ”‚  â€¢ Hybrid V2 (Gated)   â”‚
â”‚                 â”‚  â€¢ ViSoBERT     â”‚  â€¢ Soft Voting         â”‚
â”‚                 â”‚                 â”‚  â€¢ Weighted Ensemble   â”‚
â”‚                 â”‚                 â”‚  â€¢ Stacking V2         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1.2. Chi tiáº¿t cÃ¡c mÃ´ hÃ¬nh Deep Learning

### 1.2.1. PhoBERT (Base & Large)

**LÃ½ do chá»n PhoBERT:**
- PhoBERT lÃ  mÃ´ hÃ¬nh **BERT pre-trained Ä‘áº§u tiÃªn vÃ  tá»‘t nháº¥t** cho tiáº¿ng Viá»‡t (Nguyen & Nguyen, 2020).
- ÄÆ°á»£c huáº¥n luyá»‡n trÃªn 20GB vÄƒn báº£n tiáº¿ng Viá»‡t tá»« Wikipedia vÃ  bÃ¡o chÃ­.
- Sá»­ dá»¥ng tokenizer **RDRSegmenter** Ä‘á»ƒ tÃ¡ch tá»« tiáº¿ng Viá»‡t chÃ­nh xÃ¡c.

**Cáº¥u trÃºc:**
| PhiÃªn báº£n | Sá»‘ tham sá»‘ | Layers | Hidden Size | Attention Heads |
|-----------|------------|--------|-------------|-----------------|
| PhoBERT-base | 135M | 12 | 768 | 12 |
| PhoBERT-large | 370M | 24 | 1024 | 16 |

**Hyperparameters huáº¥n luyá»‡n:**

```python
# models/config.py
"phobert-base": {
    "pretrained_name": "vinai/phobert-base",
    "batch_size": 8,                    # Batch nhá» vá»«a pháº£i
    "gradient_accumulation_steps": 2,   # Effective batch = 16
    "learning_rate": 2e-5,              # LR chuáº©n cho fine-tuning BERT
}

"phobert-large": {
    "pretrained_name": "vinai/phobert-large",
    "batch_size": 4,                    # Giáº£m vÃ¬ model lá»›n hÆ¡n
    "gradient_accumulation_steps": 4,   # Effective batch = 16
    "learning_rate": 1.5e-5,            # LR tháº¥p hÆ¡n cho large model
}

# Training config
TRAINING_CONFIG = {
    "num_epochs": 5,          # Äá»§ Ä‘á»ƒ convergence
    "warmup_steps": 500,      # Warmup ~4% of training steps
    "weight_decay": 0.01,     # L2 regularization
    "max_length": 128,        # Äá»§ cho cÃ¢u Ä‘Ã¡nh giÃ¡ (avg ~30 tokens)
}
```

**Giáº£i thÃ­ch lá»±a chá»n hyperparameters:**

| Parameter | GiÃ¡ trá»‹ | LÃ½ do |
|-----------|---------|-------|
| `learning_rate = 2e-5` | TiÃªu chuáº©n BERT | Theo paper gá»‘c BERT, LR 2e-5 Ä‘áº¿n 5e-5 cho fine-tuning |
| `batch_size = 8-16` | Effective 16 | CÃ¢n báº±ng giá»¯a á»•n Ä‘á»‹nh gradient vÃ  bá»™ nhá»› GPU |
| `epochs = 5` | | Äá»§ Ä‘á»ƒ model há»™i tá»¥ mÃ  khÃ´ng overfitting |
| `warmup_steps = 500` | ~4% training | TrÃ¡nh learning rate quÃ¡ cao lÃºc Ä‘áº§u gÃ¢y phÃ¡ vá»¡ pre-trained weights |
| `weight_decay = 0.01` | | Regularization chuáº©n cho Transformer |
| `max_length = 128` | | CÃ¢u Ä‘Ã¡nh giÃ¡ sinh viÃªn thÆ°á»ng ngáº¯n (~30 tokens), 128 lÃ  dÆ° |

### 1.2.2. ViSoBERT

**LÃ½ do chá»n ViSoBERT:**
- ViSoBERT Ä‘Æ°á»£c train trÃªn **vÄƒn báº£n social media tiáº¿ng Viá»‡t** (Facebook, YouTube comments).
- Ká»³ vá»ng: Hiá»ƒu tá»‘t hÆ¡n ngÃ´n ngá»¯ teen code, viáº¿t táº¯t, emoji.

**Káº¿t quáº£ thá»±c táº¿:** 
- ViSoBERT (91.19%) < PhoBERT-base (93.46%).
- **Giáº£i thÃ­ch:** Dataset UIT-VSFC lÃ  feedback chÃ­nh thá»©c cá»§a sinh viÃªn, Ã­t teen code hÆ¡n social media. PhoBERT train trÃªn vÄƒn báº£n chuáº©n phÃ¹ há»£p hÆ¡n.

---

## 1.3. Traditional Machine Learning

### 1.3.1. SVM vá»›i TF-IDF

**Pipeline:**
```
Text â†’ TF-IDF Vectorizer (5000 features) â†’ LinearSVC (balanced)
```

**TF-IDF Parameters:**
```python
TfidfVectorizer(
    max_features=5000,      # Giá»›i háº¡n vocabulary
    ngram_range=(1, 2),     # Unigram + Bigram
    sublinear_tf=True,      # Logarithmic scaling: 1 + log(tf)
    min_df=3                # Bá» tá»« xuáº¥t hiá»‡n < 3 láº§n
)
```

**LÃ½ do chá»n LinearSVC:**
- SVM vá»›i kernel linear nhanh vÃ  hiá»‡u quáº£ vá»›i vÄƒn báº£n.
- `class_weight='balanced'` tá»± Ä‘á»™ng cÃ¢n báº±ng lá»›p thiá»ƒu sá»‘.

**Káº¿t quáº£:** 89.73% accuracy - **Baseline máº¡nh** cho so sÃ¡nh.

---

## 1.4. Hybrid Models

### 1.4.1. Hybrid V1 (Feature Fusion)

**Kiáº¿n trÃºc:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PhoBERT [CLS] â”‚â”€â”€â”€â”
â”‚  (768 dim)     â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                     â”œâ”€â”€â–º Concatenate â”€â”€â–º MLP â”€â”€â–º Softmax
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  TF-IDF        â”‚â”€â”€â”€â”¤
â”‚  (5000 dim)    â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  Handcrafted   â”‚â”€â”€â”€â”˜
â”‚  (10 features) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Handcrafted Features (10 dimensions):**
1. Sá»‘ tá»« tÃ­ch cá»±c (positive word count)
2. Sá»‘ tá»« tiÃªu cá»±c (negative word count)
3. Sá»‘ tá»« phá»§ Ä‘á»‹nh (negation count)
4. Sá»‘ emoji
5. Sá»‘ dáº¥u cháº¥m than (!)
6. Sá»‘ dáº¥u há»i (?)
7. Tá»‰ lá»‡ tá»« tÃ­ch cá»±c
8. Tá»‰ lá»‡ tá»« tiÃªu cá»±c
9. CÃ³ emoji tÃ­ch cá»±c khÃ´ng (0/1)
10. Polarity score = (pos - neg) / (pos + neg)

**Káº¿t quáº£:** 89.17% - Tháº¥p hÆ¡n PhoBERT thuáº§n do:
- BERT embeddings bá»‹ **freeze** (khÃ´ng fine-tune).
- Concatenation Ä‘Æ¡n giáº£n khÃ´ng tá»‘i Æ°u.

### 1.4.2. Hybrid V2 (Gated Fusion)

**Cáº£i tiáº¿n so vá»›i V1:**

1. **Unfreeze last 2 layers cá»§a PhoBERT** â†’ Cho phÃ©p fine-tuning nháº¹.
2. **Gated Fusion Mechanism:**
```python
gate = sigmoid(W Ã— [BERT_emb, Feature_emb])  # gate âˆˆ [0,1]
fused = gate * BERT_emb + (1 - gate) * Feature_emb
```

**Ã nghÄ©a:** Model tá»± há»c khi nÃ o nÃªn tin BERT (gate â‰ˆ 1), khi nÃ o nÃªn tin Features (gate â‰ˆ 0).

**Káº¿t quáº£ Ä‘áº·c biá»‡t:**
- Accuracy: 88.76% (tháº¥p hÆ¡n V1)
- **Neutral Recall: 76%** (cao nháº¥t trong táº¥t cáº£ models!)
- **Insight:** Gated fusion giÃºp model báº¯t Ä‘Æ°á»£c nhiá»u cÃ¢u Neutral hÆ¡n báº±ng cÃ¡ch dá»±a vÃ o handcrafted features khi BERT khÃ´ng cháº¯c cháº¯n.

---

## 1.5. Ensemble Methods

### 1.5.1. Soft Voting

**CÃ´ng thá»©c:**
```
P_ensemble(class) = (1/N) Ã— Î£ P_model_i(class)
prediction = argmax(P_ensemble)
```

**Táº¡i sao chá»n Soft Voting thay vÃ¬ Hard Voting:**
- Soft voting sá»­ dá»¥ng **xÃ¡c suáº¥t** (continuous) thay vÃ¬ chá»‰ nhÃ£n (discrete).
- Lá»£i dá»¥ng Ä‘Æ°á»£c **Ä‘á»™ tá»± tin** cá»§a tá»«ng model.

### 1.5.2. Weighted Ensemble

**CÃ´ng thá»©c:**
```
weights = [0.4, 0.3, 0.3]  # PhoBERT-large, PhoBERT-base, ViSoBERT
P_ensemble = Î£ weight_i Ã— P_model_i
```

**LÃ½ do chá»n trá»ng sá»‘:**
- PhoBERT-large cÃ³ accuracy cao nháº¥t â†’ trá»ng sá»‘ cao nháº¥t.
- CÃ¡c model khÃ¡c Ä‘Ã³ng gÃ³p Ä‘á»ƒ tÄƒng diversity.

### 1.5.3. Stacking V2 (Meta-Learning)

**Kiáº¿n trÃºc 2 táº§ng:**
```
Level 0: [PhoBERT-large, PhoBERT-base] â†’ Predictions on Dev set
                     â†“
Level 1: Logistic Regression (Meta-Learner) â†’ Final prediction
```

**Táº¡i sao dÃ¹ng Stacking:**
- Meta-learner **há»c** cÃ¡ch káº¿t há»£p tá»‘i Æ°u tá»« dá»¯ liá»‡u.
- CÃ³ thá»ƒ phÃ¡t hiá»‡n pattern: "Khi model A tá»± tin nhÆ°ng model B khÃ´ng â†’ cÃ³ thá»ƒ sai".

---

## 1.6. Xá»­ lÃ½ Class Imbalance

**Váº¥n Ä‘á»:** Lá»›p Neutral chá»‰ chiáº¿m **5.3%** dá»¯ liá»‡u.

**Giáº£i phÃ¡p: Class-Weighted Loss**

```python
def compute_class_weights(labels):
    label_counts = Counter(labels)  # {0: 5077, 1: 594, 2: 5755}
    total = sum(label_counts.values())
    weights = [total / (3 Ã— count) for count in label_counts]
    return weights  # [0.75, 6.45, 0.66]
```

**Ã nghÄ©a:** Neutral Ä‘Æ°á»£c "pháº¡t náº·ng" hÆ¡n náº¿u sai (weight = 6.45 so vá»›i 0.7).

**Káº¿t quáº£:**
- Neutral F1: 0.40 (khÃ´ng weight) â†’ **0.61** (cÃ³ weight) = **+52.5% improvement**

---

# PHáº¦N 2: Káº¾T QUáº¢ PHÃ‚N TÃCH QUAN ÄIá»‚M

## 2.1. Báº£ng so sÃ¡nh tá»•ng há»£p

| Category | Model | Accuracy | Weighted F1 | Macro F1 | Neutral F1 |
|----------|-------|----------|-------------|----------|------------|
| Baseline | Nguyen et al. (2018) | ~87% | - | ~75% | - |
| | | | | | |
| **Single** | **PhoBERT-large** | **93.56%** | 0.932 | 0.826 | 0.576 |
| **Single** | **PhoBERT-base** â­ | **93.46%** | **0.933** | **0.837** | **0.610** |
| Single | ViSoBERT | 91.19% | 0.906 | 0.767 | 0.436 |
| Single | SVM + TF-IDF | 89.73% | 0.893 | 0.746 | 0.399 |
| | | | | | |
| **Ensemble** | **Soft Voting** â­ | **92.61%** | 0.923 | 0.806 | 0.530 |
| Ensemble | Weighted Ensemble | 92.55% | 0.924 | 0.817 | 0.562 |
| Ensemble | Stacking V2 | 91.38% | 0.918 | 0.801 | 0.525 |
| Ensemble | Majority Voting | 91.16% | 0.908 | 0.782 | 0.483 |
| | | | | | |
| Hybrid | Hybrid V1: BERT + TF-IDF + Lexicon (Concat) | 89.17% | 0.890 | 0.747 | 0.406 |
| Hybrid | Hybrid V2: Fine-tune + Gated Fusion | 88.76% | 0.900 | 0.779 | 0.491 |

> â­ **Best Single:** PhoBERT-base (93.46%)  
> â­ **Best Ensemble:** Soft Voting (92.61%)  
> ğŸ’¡ **Key Finding:** Single PhoBERT outperforms Ensemble methods

## 2.2. PhÃ¢n tÃ­ch chi tiáº¿t model tá»‘t nháº¥t

### PhoBERT-base (Best Weighted F1: 0.933)

| Class | Precision | Recall | F1-Score | Support | Giáº£i thÃ­ch |
|-------|-----------|--------|----------|---------|------------|
| Negative | 0.939 | 0.962 | **0.950** | 1,409 | Ráº¥t tá»‘t - nhiá»u tá»« khÃ³a rÃµ rÃ ng |
| Neutral | 0.703 | 0.539 | **0.610** | 167 | Precision cao nhÆ°ng Recall tháº¥p |
| Positive | 0.949 | 0.952 | **0.951** | 1,590 | Ráº¥t tá»‘t - nhiá»u tá»« khÃ³a rÃµ rÃ ng |

### Confusion Matrix Analysis

```
              Predicted
              Neg   Neu   Pos
Actual  Neg  1355   27    27     â† 96.2% Ä‘Ãºng
        Neu   49    90    28     â† 53.9% Ä‘Ãºng (tháº¥p!)
        Pos   40    36   1514    â† 95.2% Ä‘Ãºng
```

**PhÃ¢n tÃ­ch:**
- **Neutral bá»‹ nháº§m thÃ nh Negative (49 máº«u):** CÃ¢u chá»©a tá»« tiÃªu cá»±c nhÆ°ng mang nghÄ©a trung láº­p.
- **Neutral bá»‹ nháº§m thÃ nh Positive (28 máº«u):** CÃ¢u chá»©a tá»« tÃ­ch cá»±c nháº¹.

## 2.3. So sÃ¡nh vá»›i Baseline

| Metric | Baseline (2018) | Best (PhoBERT-base) | Improvement |
|--------|-----------------|---------------------|-------------|
| Accuracy | ~87% | **93.46%** | **+6.46%** |
| Macro F1 | ~75% | **83.70%** | **+8.70%** |

**Giáº£i thÃ­ch improvement:**
1. **Pre-trained Language Model:** PhoBERT Ä‘Ã£ há»c ngá»¯ nghÄ©a tiáº¿ng Viá»‡t tá»« 20GB text.
2. **Fine-tuning thay vÃ¬ Feature Engineering:** Model tá»± há»c representation tá»‘t hÆ¡n TF-IDF.
3. **Contextual Embeddings:** "Hay" trong "bÃ i giáº£ng hay" â‰  "hay" trong "hay than phiá»n".

---

# PHáº¦N 3: PHÃ‚N TÃCH Lá»–I, Æ¯U ÄIá»‚M, Háº N CHáº¾

## 3.1. Æ¯u Ä‘iá»ƒm

| Æ¯u Ä‘iá»ƒm | Minh chá»©ng | Ã nghÄ©a thá»±c tiá»…n |
|---------|------------|-------------------|
| **Accuracy cao** | 93.46% (vs 87% baseline) | CÃ³ thá»ƒ triá»ƒn khai thá»±c táº¿ |
| **Robust vá»›i lá»›p Ä‘a sá»‘** | Neg/Pos F1 > 0.95 | ÄÃ¡ng tin cáº­y cho Ä‘Ã¡nh giÃ¡ tÃ­ch cá»±c/tiÃªu cá»±c |
| **Transfer Learning hiá»‡u quáº£** | Chá»‰ cáº§n 11K máº«u Ä‘áº¡t 93%+ | Tiáº¿t kiá»‡m dá»¯ liá»‡u vÃ  thá»i gian |
| **Class-weighted cáº£i thiá»‡n minority** | Neutral F1: 0.40 â†’ 0.61 | Giáº£m bias vá»›i lá»›p thiá»ƒu sá»‘ |

## 3.2. Háº¡n cháº¿

### 3.2.1. Lá»›p Neutral váº«n yáº¿u

**Thá»‘ng kÃª:**
- Neutral F1 = 0.61 (so vá»›i 0.95 cá»§a Neg/Pos)
- Recall = 0.54 â†’ **Bá» sÃ³t 46% cÃ¢u Neutral**

**NguyÃªn nhÃ¢n gá»‘c rá»…:**
1. **Data Imbalance:** Chá»‰ 594/11,426 = 5.2% máº«u train lÃ  Neutral.
2. **Báº£n cháº¥t mÆ¡ há»“:** CÃ¢u Neutral thÆ°á»ng khÃ´ng cÃ³ tá»« khÃ³a sentiment rÃµ rÃ ng.
3. **Label noise:** Ranh giá»›i Neutral/Positive ráº¥t chá»§ quan.

**VÃ­ dá»¥ cÃ¢u Neutral bá»‹ nháº§m:**

| CÃ¢u | Predicted | Actual | LÃ½ do sai |
|-----|-----------|--------|-----------|
| "Tháº§y dáº¡y Ä‘Æ°á»£c" | Positive | Neutral | "Ä‘Æ°á»£c" = slightly positive |
| "MÃ´n nÃ y bÃ¬nh thÆ°á»ng" | Positive | Neutral | "bÃ¬nh thÆ°á»ng" ambiguous |
| "Cáº§n cáº£i thiá»‡n thÃªm" | Negative | Neutral | "cáº£i thiá»‡n" sounds negative |

### 3.2.2. Ensemble khÃ´ng vÆ°á»£t Single Model

**Quan sÃ¡t:** Soft Voting (92.61%) < PhoBERT-base (93.46%)

**Giáº£i thÃ­ch:**
1. **Weak models kÃ©o xuá»‘ng:** ViSoBERT (91.19%), SVM (89.73%) Ä‘Ã³ng gÃ³p noise.
2. **Thiáº¿u diversity:** Táº¥t cáº£ Transformer models cÃ³ patterns tÆ°Æ¡ng tá»±.
3. **Simple averaging:** KhÃ´ng há»c trá»ng sá»‘ tá»« dá»¯ liá»‡u.

**BÃ i há»c:** Ensemble hiá»‡u quáº£ khi cÃ¡c base models cÃ³ **diverse errors**, khÃ´ng pháº£i khi chÃºng giá»‘ng nhau.

### 3.2.3. Hybrid chÆ°a vÆ°á»£t Fine-tuning

**Quan sÃ¡t:** Hybrid V2 (88.76%) < PhoBERT (93.46%)

**Giáº£i thÃ­ch:**
1. **End-to-end fine-tuning máº¡nh hÆ¡n:** 135M parameters Ä‘Æ°á»£c optimize cÃ¹ng nhau.
2. **Handcrafted features limited:** Con ngÆ°á»i khÃ´ng thiáº¿t káº¿ Ä‘Æ°á»£c features tá»‘t báº±ng deep learning tá»± há»c.
3. **Information bottleneck:** Concatenation cÃ³ thá»ƒ lÃ m máº¥t thÃ´ng tin.

---

# PHáº¦N 4: CÃC ÄIá»‚M Má»šI, SÃNG Táº O

## 4.1. PhÃ¡t hiá»‡n báº¥t ngá»

### 4.1.1. PhoBERT-base > PhoBERT-large

| Model | Parameters | Accuracy | Weighted F1 |
|-------|------------|----------|-------------|
| PhoBERT-base | 135M | 93.46% | **0.933** |
| PhoBERT-large | 370M | 93.56% | 0.932 |

**PhÃ¢n tÃ­ch:**
- Accuracy chÃªnh lá»‡ch khÃ´ng Ä‘Ã¡ng ká»ƒ (0.1%).
- **Weighted F1 cá»§a base cao hÆ¡n** (0.933 vs 0.932).
- **Neutral F1:** base = 0.610 > large = 0.576.

**Giáº£i thÃ­ch:**
1. **Overfitting:** Dataset 16K máº«u chÆ°a Ä‘á»§ Ä‘á»ƒ exploit 370M params cá»§a large model.
2. **Regularization hiá»‡u quáº£ hÆ¡n:** Model nhá» hÆ¡n Ã­t bá»‹ overfit.
3. **Compute tradeoff:** Large tá»‘n 3x thá»i gian nhÆ°ng khÃ´ng tá»‘t hÆ¡n.

**Recommendation:** Vá»›i dataset < 50K máº«u, nÃªn dÃ¹ng PhoBERT-base.

### 4.1.2. Gated Fusion cá»±c tá»‘t cho Neutral Recall

| Model | Neutral Precision | Neutral Recall | Neutral F1 |
|-------|-------------------|----------------|------------|
| PhoBERT-base | **0.703** | 0.539 | 0.610 |
| Hybrid V2 (Gated) | 0.363 | **0.760** | 0.491 |

**Insight:**
- Hybrid V2 **báº¯t Ä‘Æ°á»£c 76% cÃ¢u Neutral** (vs 54% cá»§a PhoBERT).
- Trade-off: Precision tháº¥p hÆ¡n (0.36 vs 0.70).
- **Use case:** Náº¿u cáº§n tÃ¬m Táº¤T Cáº¢ cÃ¢u Neutral (screening), dÃ¹ng Hybrid V2.

**CÆ¡ cháº¿:** Gate tá»± Ä‘á»™ng "táº¯t" BERT khi khÃ´ng cháº¯c cháº¯n, dá»±a vÃ o features ngÃ´n ngá»¯ thay tháº¿.

## 4.2. ÄÃ³ng gÃ³p ká»¹ thuáº­t

| ÄÃ³ng gÃ³p | MÃ´ táº£ | Impact |
|----------|-------|--------|
| **Multi-model Benchmark** | So sÃ¡nh 9 models trÃªn cÃ¹ng dataset | Baseline cho nghiÃªn cá»©u tÆ°Æ¡ng lai |
| **Gated Feature Fusion** | Tá»± Ä‘á»™ng cÃ¢n báº±ng BERT vs Features | Cáº£i thiá»‡n Neutral Recall 41% |
| **Class-Weighted BERT** | Ãp dá»¥ng weighted loss cho Transformer | Neutral F1 tÄƒng 52.5% |
| **Stacking for NLP** | Meta-learning ensemble cho sentiment | Pipeline cÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng |

## 4.3. HÆ°á»›ng phÃ¡t triá»ƒn

1. **Data Augmentation cho Neutral:**
   - Back-translation (Viá»‡t â†’ Anh â†’ Viá»‡t)
   - Paraphrase vá»›i LLM (GPT, Gemini)

2. **Contrastive Learning:**
   - Há»c embeddings phÃ¢n biá»‡t rÃµ Neutral vs Positive/Negative.

3. **Multi-task Learning:**
   - Káº¿t há»£p Sentiment + Topic classification â†’ shared representation tá»‘t hÆ¡n.

4. **Few-shot vá»›i LLM:**
   - DÃ¹ng GPT-4/Gemini vá»›i few-shot prompting cho Neutral detection.

---

# Káº¾T LUáº¬N

Dá»± Ã¡n Ä‘Ã£ triá»ƒn khai thÃ nh cÃ´ng **9 phÆ°Æ¡ng phÃ¡p** phÃ¢n tÃ­ch quan Ä‘iá»ƒm, Ä‘áº¡t káº¿t quáº£ **93.46% accuracy** (vÆ°á»£t baseline 6.46%). CÃ¡c Ä‘Ã³ng gÃ³p chÃ­nh:

1. **Benchmark toÃ n diá»‡n** cÃ¡c phÆ°Æ¡ng phÃ¡p tá»« traditional ML Ä‘áº¿n deep learning.
2. **PhÃ¡t hiá»‡n quan trá»ng:** PhoBERT-base hiá»‡u quáº£ hÆ¡n large cho dataset nhá».
3. **Ká»¹ thuáº­t má»›i:** Gated Fusion cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Neutral Recall.
4. **Best practice:** Class-weighted loss lÃ  báº¯t buá»™c cho imbalanced sentiment data.

**ThÃ¡ch thá»©c cÃ²n láº¡i:** Lá»›p Neutral váº«n lÃ  Ä‘iá»ƒm yáº¿u (F1 = 0.61) cáº§n nghiÃªn cá»©u thÃªm.

---

**NgÃ y hoÃ n thÃ nh:** ThÃ¡ng 2, 2026
